{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers\n",
    "!pip install torchtext\n",
    "!pip install pytorch_lightning\n",
    "!pip install datasets\n",
    "!pip install tensorboard\n",
    "!pip install lion_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The loss didnt decrease much and was equal to nan many times which disturbed the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_DB_AMP_OCP import train_model, get_model, get_ds\n",
    "from config_file import get_config, get_weights_file_path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "config = get_config()\n",
    "config[\"batch_size\"] = 24\n",
    "config[\"preload\"] = None\n",
    "config[\"num_epochs\"] = 18\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.cuda.amp.autocast(enabled=True)\n",
    "\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "#Tensorboard\n",
    "writer = SummaryWriter(config[\"experiment_name\"])\n",
    "\n",
    "#Adam is used to train each feature with a different learning rate. \n",
    "#If some feature is appearing less, adam takes care of it\n",
    "optimizer = Lion(model.parameters(), lr = 1e-4/10, weight_decay = 1e-2)\n",
    "\n",
    "initial_epoch = 0\n",
    "global_step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LR = 10**-3 ## 10 times the value it is decided in the paper\n",
    "STEPS_PER_EPOCH = len(train_dataloader)\n",
    "EPOCHS = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=MAX_LR, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,\n",
    "                                                div_factor=100, three_phase=False, pct_start=int(0.3*EPOCHS) / EPOCHS,\n",
    "                                                anneal_strategy='linear', final_div_factor=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1)\n",
    "Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "# if config[\"preload\"]:\n",
    "#     model_filename = get_weights_file_path(config, config[\"preload\"])\n",
    "#     print(\"Preloading model {model_filename}\")\n",
    "#     state = torch.load(model_filename)\n",
    "#     model.load_state_dict(state[\"model_state_dict\"])\n",
    "#     initial_epoch = state[\"epoch\"] + 1\n",
    "#     optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "#     global_step = state[\"global_step\"]\n",
    "#     print(\"preloaded\")\n",
    "        \n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "lr = [0.0]\n",
    "\n",
    "for epoch in range(initial_epoch, config[\"num_epochs\"]):\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(epoch)\n",
    "    model.train()\n",
    "    batch_iterator = tqdm(train_dataloader, desc = f\"Processing Epoch {epoch:02d}\")\n",
    "    \n",
    "    loss_list = []\n",
    "    \n",
    "    for batch in batch_iterator:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        encoder_input = batch[\"encoder_input\"].to(device)\n",
    "        decoder_input = batch[\"decoder_input\"].to(device)\n",
    "        encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "        decoder_mask = batch[\"decoder_mask\"].to(device)\n",
    "        \n",
    "        with torch.autocast(device_type='cuda',dtype = torch.float16 ):\n",
    "        \n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "        \n",
    "            label = batch[\"label\"].to(device)\n",
    "        \n",
    "        #Compute loss using cross entropy\n",
    "            tgt_vocab_size = tokenizer_tgt.get_vocab_size()\n",
    "            loss = loss_fn(proj_output.view(-1, tgt_vocab_size), label.view(-1))\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "        batch_iterator.set_postfix({\"recent loss\": f\"{loss.item():6.3f}\" , \"avg loss\": f\"{sum(loss_list)/len(loss_list):6.3f}\", 'lr' : f'{lr[-1]}'} )\n",
    "\n",
    "        #Log the loss\n",
    "        writer.add_scalar('train_loss', loss.item(), global_step)\n",
    "        writer.flush()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        #Backpropogate loss\n",
    "        # loss.backward()\n",
    "        scale = scaler.get_scale()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        skip_lr_sched = (scale > scaler.get_scale())    \n",
    "        if not skip_lr_sched:\n",
    "            scheduler.step()\n",
    "        lr.append(scheduler.get_last_lr())\n",
    "        \n",
    "        \n",
    "        \n",
    "        # #Update weights\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad(set_to_none=True)\n",
    "        global_step+=1\n",
    "        \n",
    "    #run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, writer, global_step)\n",
    "    \n",
    "    \n",
    "    model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"global_step\": global_step\n",
    "        },\n",
    "        model_filename\n",
    "    )\n",
    "    print(\"loss for this epoch is \", sum(loss_list)/len(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erav2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
