{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tokenizers) (0.23.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: requests in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.3.1)\n",
      "Requirement already satisfied: filelock in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.14.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/shreyash/Developer/TSAI/ERAV2/erav2/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchtext in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (0.17.2)\n",
      "Requirement already satisfied: torch==2.2.2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torchtext) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torchtext) (1.24.4)\n",
      "Requirement already satisfied: tqdm in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torchtext) (4.66.4)\n",
      "Requirement already satisfied: requests in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: filelock in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (3.14.0)\n",
      "Requirement already satisfied: fsspec in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (3.1.4)\n",
      "Requirement already satisfied: sympy in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests->torchtext) (2024.6.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests->torchtext) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests->torchtext) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from jinja2->torch==2.2.2->torchtext) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from sympy->torch==2.2.2->torchtext) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/shreyash/Developer/TSAI/ERAV2/erav2/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pytorch_lightning in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (2.2.5)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pytorch_lightning) (4.66.4)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pytorch_lightning) (0.11.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pytorch_lightning) (1.4.0.post0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pytorch_lightning) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pytorch_lightning) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pytorch_lightning) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pytorch_lightning) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pytorch_lightning) (4.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2022.5.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pytorch_lightning) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (56.0.0)\n",
      "Requirement already satisfied: sympy in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch>=1.13.0->pytorch_lightning) (1.12.1)\n",
      "Requirement already satisfied: filelock in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch>=1.13.0->pytorch_lightning) (3.14.0)\n",
      "Requirement already satisfied: networkx in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch>=1.13.0->pytorch_lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from torch>=1.13.0->pytorch_lightning) (3.1.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from jinja2->torch>=1.13.0->pytorch_lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from sympy->torch>=1.13.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.7)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/shreyash/Developer/TSAI/ERAV2/erav2/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (2.19.2)\n",
      "Requirement already satisfied: requests>=2.32.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: packaging in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: multiprocess in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (0.23.3)\n",
      "Requirement already satisfied: pandas in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: xxhash in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: filelock in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests>=2.32.1->datasets) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests>=2.32.1->datasets) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/shreyash/Developer/TSAI/ERAV2/erav2/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorboard in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (2.14.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (2.30.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (5.27.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (56.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (1.64.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (0.43.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from tensorboard) (1.24.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (7.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.19.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/shreyash/Developer/TSAI/ERAV2/erav2/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers\n",
    "!pip install torchtext\n",
    "!pip install pytorch_lightning\n",
    "!pip install datasets\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import math\n",
    "\n",
    "# class LayerNormalization(nn.Module):\n",
    "\n",
    "#     def __init__(self, eps:float=10**-6) -> None:\n",
    "#         super().__init__()\n",
    "#         self.eps = eps\n",
    "#         self.alpha = nn.Parameter(torch.ones(1)) # alpha is a learnable parameter\n",
    "#         self.bias = nn.Parameter(torch.zeros(1)) # bias is a learnable parameter\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (batch, seq_len, hidden_size)\n",
    "#         # Keep the dimension for broadcasting\n",
    "#         mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "#         # Keep the dimension for broadcasting\n",
    "#         std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "#         # eps is to prevent dividing by zero or when std is very small\n",
    "#         return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "# class FeedForwardBlock(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "#         super().__init__()\n",
    "#         self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "#         return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "# class InputEmbeddings(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "#         # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "#         return self.embedding(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "# class PositionalEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.seq_len = seq_len\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         # Create a matrix of shape (seq_len, d_model)\n",
    "#         pe = torch.zeros(seq_len, d_model)\n",
    "#         # Create a vector of shape (seq_len)\n",
    "#         position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "#         # Create a vector of shape (d_model)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "#         # Apply sine to even indices\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "#         # Apply cosine to odd indices\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "#         # Add a batch dimension to the positional encoding\n",
    "#         pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "#         # Register the positional encoding as a buffer\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# class ResidualConnection(nn.Module):\n",
    "    \n",
    "#         def __init__(self, dropout: float) -> None:\n",
    "#             super().__init__()\n",
    "#             self.dropout = nn.Dropout(dropout)\n",
    "#             self.norm = LayerNormalization()\n",
    "    \n",
    "#         def forward(self, x, sublayer):\n",
    "#             return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "# class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model # Embedding vector size\n",
    "#         self.h = h # Number of heads\n",
    "#         # Make sure d_model is divisible by h\n",
    "#         assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "#         self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "#         self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "#         self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "#         self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "#         self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "#         d_k = query.shape[-1]\n",
    "#         # Just apply the formula from the paper\n",
    "#         # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "#         attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "#         #S16\n",
    "#         _MASKING_VALUE=-1e30 if attention_scores.dtype==torch.float32 else -1e4\n",
    "#         if mask is not None:\n",
    "#             # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "#             # attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "#             # print('attetion score matrix size',attention_scores.shape, \"mask shape\",  mask.shape)\n",
    "#             attention_scores.masked_fill_(mask==0, value=_MASKING_VALUE)\n",
    "#         attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "#         if dropout is not None:\n",
    "#             attention_scores = dropout(attention_scores)\n",
    "#         # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "#         # return attention scores which can be used for visualization\n",
    "#         return (attention_scores @ value), attention_scores\n",
    "\n",
    "#     def forward(self, q, k, v, mask):\n",
    "#         query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "#         key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "#         value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "#         # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "#         query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "#         key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "#         value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "#         # Calculate attention\n",
    "#         x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "#         # Combine all the heads together\n",
    "#         # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "#         x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "#         # Multiply by Wo\n",
    "#         # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "#         return self.w_o(x)\n",
    "\n",
    "# class EncoderBlock(nn.Module):\n",
    "\n",
    "#     def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "#         super().__init__()\n",
    "#         self.self_attention_block = self_attention_block\n",
    "#         self.feed_forward_block = feed_forward_block\n",
    "#         self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "#     def forward(self, x, src_mask):\n",
    "#         x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "#         x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "#         return x\n",
    "    \n",
    "# class Encoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, layers: nn.ModuleList) -> None:\n",
    "#         super().__init__()\n",
    "#         self.layers = layers\n",
    "#         self.norm = LayerNormalization()\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, mask)\n",
    "#         return self.norm(x)\n",
    "\n",
    "# class DecoderBlock(nn.Module):\n",
    "\n",
    "#     def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "#         super().__init__()\n",
    "#         self.self_attention_block = self_attention_block\n",
    "#         self.cross_attention_block = cross_attention_block\n",
    "#         self.feed_forward_block = feed_forward_block\n",
    "#         self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "#     def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "#         x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "#         x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "#         x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "#         return x\n",
    "    \n",
    "# class Decoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, layers: nn.ModuleList) -> None:\n",
    "#         super().__init__()\n",
    "#         self.layers = layers\n",
    "#         self.norm = LayerNormalization()\n",
    "\n",
    "#     def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "#         return self.norm(x)\n",
    "\n",
    "# class ProjectionLayer(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model, vocab_size) -> None:\n",
    "#         super().__init__()\n",
    "#         self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "#     def forward(self, x) -> None:\n",
    "#         # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "#         return torch.log_softmax(self.proj(x), dim = -1)\n",
    "    \n",
    "# class Transformer(nn.Module):\n",
    "\n",
    "#     def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.src_embed = src_embed\n",
    "#         self.tgt_embed = tgt_embed\n",
    "#         self.src_pos = src_pos\n",
    "#         self.tgt_pos = tgt_pos\n",
    "#         self.projection_layer = projection_layer\n",
    "\n",
    "#     def encode(self, src, src_mask):\n",
    "#         # (batch, seq_len, d_model)\n",
    "#         src = self.src_embed(src)\n",
    "#         src = self.src_pos(src)\n",
    "#         return self.encoder(src, src_mask)\n",
    "    \n",
    "#     def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "#         # (batch, seq_len, d_model)\n",
    "#         tgt = self.tgt_embed(tgt)\n",
    "#         tgt = self.tgt_pos(tgt)\n",
    "#         return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "#     def project(self, x):\n",
    "#         # (batch, seq_len, vocab_size)\n",
    "#         return self.projection_layer(x)\n",
    "    \n",
    "# def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "#     # Create the embedding layers\n",
    "#     src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "#     tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "#     # Create the positional encoding layers\n",
    "#     src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "#     tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "#     # Create the encoder blocks\n",
    "#     encoder_blocks = []\n",
    "#     for _ in range(N):\n",
    "#         encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "#         feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "#         encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "#         encoder_blocks.append(encoder_block)\n",
    "\n",
    "#     # Create the decoder blocks\n",
    "#     decoder_blocks = []\n",
    "#     for _ in range(N):\n",
    "#         decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "#         decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "#         feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "#         decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "#         decoder_blocks.append(decoder_block)\n",
    "    \n",
    "#     # Create the encoder and decoder\n",
    "#     encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "#     decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "#     # Create the projection layer\n",
    "#     projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "#     # Create the transformer\n",
    "#     transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "#     # Initialize the parameters\n",
    "#     for p in transformer.parameters():\n",
    "#         if p.dim() > 1:\n",
    "#             nn.init.xavier_uniform_(p)\n",
    "    \n",
    "#     return transformer\n",
    "# from dataset import BillingualDataset, casual_mask\n",
    "# from config_file import get_config, get_weights_file_path\n",
    "\n",
    "# import torchtext.datasets as datasets\n",
    "# import torch\n",
    "# torch.cuda.amp.autocast(enabled = True)\n",
    "\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# import warnings\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import WordLevel\n",
    "# from tokenizers.trainers import WordLevelTrainer\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# import torchmetrics\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:12240\"\n",
    "# config = get_config()\n",
    "\n",
    "# def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    \n",
    "    \n",
    "#     sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "#     eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "    \n",
    "#     encoder_output = model.encode(source, source_mask)\n",
    "#     #Initialize the decoder input with SOS token\n",
    "#     decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "#     while True:\n",
    "#         if decoder_input.size(1) == max_len:\n",
    "#             break\n",
    "#         decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "#         out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "        \n",
    "#         prob = model.project(out[:, -1])\n",
    "#         _, next_word = torch.max(prob, dim=1)\n",
    "#         decoder_input = torch.cat(\n",
    "#             [\n",
    "#                 decoder_input,\n",
    "#                 torch.empty(1, 1).type_as(source_mask).fill_(next_word.item()).to(device)\n",
    "#             ],\n",
    "#             dim =  1\n",
    "#         )\n",
    "        \n",
    "#         if next_word == eos_idx:\n",
    "#             break\n",
    "        \n",
    "#     return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "# def run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, max_len, device, writer, global_step):\n",
    "#     model.eval()\n",
    "#     count = 0\n",
    "#     source_texts = []\n",
    "#     expected = []\n",
    "#     predicted = []\n",
    "    \n",
    "#     try:\n",
    "#         with os.popen('stty size', 'r') as console:\n",
    "#             _, console_width = console.read().split()\n",
    "#             console_width = int(console_width)\n",
    "#     except:\n",
    "#         console_width = 80\n",
    "        \n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_dataloader:\n",
    "#             count += 1\n",
    "#             encoder_input = batch[\"encoder_input\"].to(device)\n",
    "#             encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "            \n",
    "#             assert encoder_input.size(0)==1, \"Batch size must be 1 for validation\"\n",
    "#             model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            \n",
    "#             source_text = batch[\"src_text\"][0]\n",
    "#             target_text = batch[\"tgt_text\"][0]\n",
    "#             model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "            \n",
    "#             source_texts.append(source_text)\n",
    "#             expected.append(target_text)\n",
    "#             predicted.append(model_out_text)\n",
    "#     \"\"\"        \n",
    "#             print(\"SOURCE\", source_text)\n",
    "#             print(\"TARGET\", target_text)\n",
    "#             print(\"PREDICTED\", model_out_text)\n",
    "            \n",
    "#     if writer:\n",
    "#         metric = torchmetrics.CharErrorRate()\n",
    "#         cer = metric(predicted, expected)\n",
    "#         writer.add_scalar('validation cer', cer, global_step)\n",
    "#         writer.flush()\n",
    "        \n",
    "#         metric = torchmetrics.WordErrorRate()\n",
    "#         wer = metric(predicted, expected)\n",
    "#         writer.add_scalar('validation wer', wer, global_step)\n",
    "#         writer.flush()\n",
    "        \n",
    "#         metric = torchmetrics.BLEUScore()\n",
    "#         bleu = metric(predicted, expected)\n",
    "#         writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "#         writer.flush()\n",
    "        \n",
    "#     \"\"\"   \n",
    "\n",
    "# def get_all_sentenses(ds, lang):\n",
    "#     for item in ds:\n",
    "#         yield item['translation'][lang]\n",
    "        \n",
    "# def get_or_build_tokenizer(config, ds, lang):\n",
    "#     tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n",
    "#     if not Path.exists(tokenizer_path):\n",
    "#         tokenizer = Tokenizer(WordLevel(unk_token = \"[UNK]\"))\n",
    "#         tokenizer.pre_tokenizer = Whitespace() \n",
    "#         trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[SOS]\", \"[EOS]\", \"[PAD]\"], min_frequency = 2)\n",
    "#         tokenizer.train_from_iterator(get_all_sentenses(ds, lang), trainer = trainer)\n",
    "#         tokenizer.save(str(tokenizer_path))\n",
    "#     else:\n",
    "#         tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "#     return tokenizer\n",
    "\n",
    "\n",
    "# def get_ds(config):\n",
    "    \n",
    "#     ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split = 'train')  \n",
    "    \n",
    "#     src_lang = config[\"lang_src\"]\n",
    "#     tgt_lang = config[\"lang_tgt\"]\n",
    "#     seq_len = config[\"seq_len\"]\n",
    "    \n",
    "#     # print(ds_raw[0], type(ds_raw), type(ds_raw[0]))\n",
    "    \n",
    "#     tokenizer_src = get_or_build_tokenizer(config, ds_raw, src_lang)\n",
    "#     tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, tgt_lang)\n",
    "    \n",
    "#     # ds_raw = sorted(ds_raw, key=lambda x: len(tokenizer_src.encode(x['translation'][src_lang]).ids))\n",
    "#     # print(ds_raw[0], type(ds_raw), type(ds_raw[0]))\n",
    "    \n",
    "#     train_ds_size = int(0.9 * len(ds_raw))\n",
    "#     val_ds_size = len(ds_raw) - train_ds_size\n",
    "#     train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "    \n",
    "#     # print(type(ds_raw), train_ds_raw[0], type(train_ds_raw), type(train_ds_raw[0]))\n",
    "    \n",
    "#     # train_ds_list = []\n",
    "    \n",
    "#     # for item in train_ds_raw:\n",
    "#     #     train_ds_list.append((src_ids, tgt_ids))\n",
    "    \n",
    "#     # train_ds_raw.sort(key=lambda x: len(tokenizer_src.encode(x['translation'][src_lang]).ids))\n",
    "    \n",
    "    \n",
    "\n",
    "#     train_ds = BillingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len)\n",
    "#     val_ds = BillingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len)\n",
    "    \n",
    "#     # print(train_ds[0], train_ds, type(train_ds), type(train_ds[0]))\n",
    "    \n",
    "#     max_len_src = 0\n",
    "#     max_len_tgt = 0\n",
    "    \n",
    "#     for item in ds_raw:\n",
    "#         src_ids = tokenizer_src.encode(item['translation'][src_lang]).ids\n",
    "#         tgt_ids = tokenizer_tgt.encode(item['translation'][tgt_lang]).ids\n",
    "#         max_len_src = max(max_len_src, len(src_ids))\n",
    "#         max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "    \n",
    "#     print(f\"Max length of the source sentence : {max_len_src}\")\n",
    "#     print(f\"Max length of the source target : {max_len_tgt}\")\n",
    "    \n",
    "#     train_dataloader = DataLoader(train_ds, batch_size = config[\"batch_size\"], shuffle = True,  collate_fn=collate_func)\n",
    "#     # print(\"Train dataloader created\", next(iter(train_dataloader)))\n",
    "#     val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
    "    \n",
    "#     return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "# def collate_func(batch):\n",
    "#     max_encoder_len = 0\n",
    "#     max_decoder_len = 0\n",
    "#     for b  in batch:\n",
    "#         max_encoder_len = max(max_encoder_len, len(b[\"encoder_input_without_padding\"]))\n",
    "#         max_decoder_len = max(max_decoder_len, len(b[\"decoder_input_without_padding\"] ))\n",
    "    \n",
    "#     seq_len_this_batch = max(max_encoder_len, max_decoder_len) + 3\n",
    "    \n",
    "#     # print(max_encoder_len, max_decoder_len, seq_len_this_batch)\n",
    "    \n",
    "#     for b in batch:\n",
    "#         enc_num_padding_tokens = seq_len_this_batch - len(b[\"encoder_input_without_padding\"] )\n",
    "#         dec_num_padding_tokens = seq_len_this_batch - len(b[\"decoder_input_without_padding\"] )\n",
    "#         # print(enc_num_padding_tokens, dec_num_padding_tokens)\n",
    "#         # print(b[\"encoder_input_without_padding\"])\n",
    "#         # print(b[\"decoder_input_without_padding\"])\n",
    "#         encoder_input = torch.cat(\n",
    "#             [\n",
    "#                 b[\"encoder_input_without_padding\"],\n",
    "#                 torch.tensor([b[\"pad_token\"]]*enc_num_padding_tokens, dtype = torch.int64)\n",
    "#             ],\n",
    "#             dim=0,\n",
    "            \n",
    "#         )\n",
    "#         decoder_input = torch.cat(\n",
    "#             [\n",
    "#                 b[\"decoder_input_without_padding\"],\n",
    "#                 torch.tensor([b[\"pad_token\"]]*dec_num_padding_tokens, dtype = torch.int64)\n",
    "#             ],\n",
    "#             dim=0\n",
    "#         )\n",
    "#         label = torch.cat(\n",
    "#             [  \n",
    "#                 b[\"label_without_padding\"],\n",
    "#                 torch.tensor([b[\"pad_token\"]]*dec_num_padding_tokens, dtype = torch.int64)\n",
    "#             ],\n",
    "#             dim=0\n",
    "#         )\n",
    "#         b[\"encoder_input\"] = encoder_input\n",
    "#         b[\"decoder_input\"] = decoder_input\n",
    "#         b[\"label\"] = label\n",
    "#         b[\"encoder_mask\"] =  ((encoder_input != b[\"pad_token\"]).unsqueeze(0)).unsqueeze(0).unsqueeze(0).int()\n",
    "#         # print(b[\"encoder_mask\"].shape)\n",
    "#         b[\"decoder_mask\"] = (decoder_input != b[\"pad_token\"]).unsqueeze(0).int() & casual_mask(decoder_input.size(0)).unsqueeze(0)\n",
    "#         # print(b[\"encoder_input\"])\n",
    "#         # print(b[\"decoder_input\"])\n",
    "#         # print(b[\"label\"])\n",
    "#         # print(b[\"encoder_mask\"])\n",
    "#         # print(b[\"decoder_mask\"])\n",
    "#         # print(b[\"src_text\"])\n",
    "#         # print(b[\"tgt_text\"])\n",
    "#         # print(\" \")\n",
    "    \n",
    "#     encoder_inputs = []\n",
    "#     decoder_inputs = []\n",
    "#     encoder_masks = []\n",
    "#     decoder_masks = []\n",
    "#     labels = []\n",
    "#     src_texts = []\n",
    "#     tgt_texts = []\n",
    "    \n",
    "#     for b in batch:\n",
    "#         encoder_inputs.append(b[\"encoder_input\"])\n",
    "#         decoder_inputs.append(b[\"decoder_input\"])\n",
    "#         encoder_masks.append(b[\"encoder_mask\"])\n",
    "#         decoder_masks.append(b[\"decoder_mask\"])\n",
    "#         labels.append(b[\"label\"])\n",
    "#         src_texts.append(b[\"src_text\"])\n",
    "#         tgt_texts.append(b[\"tgt_text\"])\n",
    "        \n",
    "#     # print(encoder_inputs)\n",
    "    \n",
    "    \n",
    "#     # for b in batch:\n",
    "        \n",
    "        \n",
    "    \n",
    "#     return {\n",
    "#         \"encoder_input\": torch.vstack(encoder_inputs),\n",
    "#         \"decoder_input\": torch.vstack(decoder_inputs),\n",
    "#         \"encoder_mask\": torch.vstack(encoder_masks),\n",
    "#         \"decoder_mask\": torch.vstack(decoder_masks),\n",
    "#         \"label\": torch.vstack(labels),\n",
    "#         \"src_text\": src_texts,\n",
    "#         \"tgt_text\": tgt_texts\n",
    "#     }\n",
    "    \n",
    "    \n",
    "\n",
    "# def get_model(config, src_vocab_size, tgt_vocab_size):\n",
    "#     model = build_transformer(src_vocab_size, tgt_vocab_size, config[\"seq_len\"], config[\"seq_len\"], d_model=config['d_model'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def train_model(config):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
    "#     print(f\"Using device : {device}\")\n",
    "    \n",
    "#     Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n",
    "#     train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "#     model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    \n",
    "#     #Tensorboard\n",
    "#     writer = SummaryWriter(config[\"experiment_name\"])\n",
    "    \n",
    "#     #Adam is used to train each feature with a different learning rate. \n",
    "#     #If some feature is appearing less, adam takes care of it\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr = config[\"lr\"], eps = 1e-9)\n",
    "    \n",
    "#     initial_epoch = 0\n",
    "#     global_step = 0\n",
    "    \n",
    "#     if config[\"preload\"]:\n",
    "#         model_filename = get_weights_file_path(config, config[\"preload\"])\n",
    "#         print(\"Preloading model {model_filename}\")\n",
    "#         state = torch.load(model_filename)\n",
    "#         model.load_state_dict(state[\"model_state_dict\"])\n",
    "#         initial_epoch = state[\"epoch\"] + 1\n",
    "#         optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "#         global_step = state[\"global_step\"]\n",
    "#         print(\"preloaded\")\n",
    "        \n",
    "#     loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1)\n",
    "    \n",
    "#     for epoch in range(initial_epoch, config[\"num_epochs\"]):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         print(epoch)\n",
    "#         model.train()\n",
    "#         batch_iterator = tqdm(train_dataloader, desc = f\"Processing Epoch {epoch:02d}\")\n",
    "        \n",
    "#         for batch in batch_iterator:\n",
    "#             encoder_input = batch[\"encoder_input\"].to(device)\n",
    "#             decoder_input = batch[\"decoder_input\"].to(device)\n",
    "#             encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "#             decoder_mask = batch[\"decoder_mask\"].to(device)\n",
    "            \n",
    "#             encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "#             decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "#             proj_output = model.project(decoder_output)\n",
    "            \n",
    "#             label = batch[\"label\"].to(device)\n",
    "            \n",
    "#             #Compute loss using cross entropy\n",
    "#             tgt_vocab_size = tokenizer_tgt.get_vocab_size()\n",
    "#             loss = loss_fn(proj_output.view(-1, tgt_vocab_size), label.view(-1))\n",
    "#             batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "#             #Log the loss\n",
    "#             writer.add_scalar('train_loss', loss.item(), global_step)\n",
    "#             writer.flush()\n",
    "            \n",
    "#             #Backpropogate loss\n",
    "#             loss.backward()\n",
    "            \n",
    "#             #Update weights\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             global_step+=1\n",
    "            \n",
    "#         #run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, writer, global_step)\n",
    "        \n",
    "        \n",
    "#         model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "#         torch.save(\n",
    "#             {\n",
    "#                 \"epoch\": epoch,\n",
    "#                 \"model_state_dict\": model.state_dict(),\n",
    "#                 \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#                 \"global_step\": global_step\n",
    "#             },\n",
    "#             model_filename\n",
    "#         )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyash/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : mps\n",
      "Max length of the source sentence : 309\n",
      "Max length of the source target : 274\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00:   0%|          | 4/1819 [00:17<2:15:13,  4.47s/it, loss=9.585] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 12.07 GB, other allocations: 6.04 GB, max allowed: 18.13 GB). Tried to allocate 253.64 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreload\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/TSAI/ERAV2/S18/train.py:309\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    306\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m#Backpropogate loss\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m#Update weights\u001b[39;00m\n\u001b[1;32m    312\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/TSAI/ERAV2/erav2/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 12.07 GB, other allocations: 6.04 GB, max allowed: 18.13 GB). Tried to allocate 253.64 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from train import train_model\n",
    "from config_file import get_config\n",
    "\n",
    "config = get_config()\n",
    "config[\"batch_size\"] = 16\n",
    "config[\"preload\"] = None\n",
    "config[\"num_epochs\"] = 18\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erav2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
